---
title: "Homework 2"
author: "42205 / Marc Gehring"
date: "2022-03-09"
output:
  pdf_document: 
    fig_height: 3
  html_notebook: default
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[C]{Applied Financial Econometrics}
- \fancyfoot[C]{\thepage}
---

# 1.

```{r, message = FALSE}
library(tidyverse)
library(sandwich)
library(haven)
library(AER)
library(modelsummary)
library(systemfonts)
```

## (a)
```{r, message = FALSE}
path = paste0(getwd(), "/pension.dta")
data = read_dta(path)
dataOrig = data
data[c("marr", "male", "p401k", "e401k")] = 
lapply(data[c("marr", "male", "p401k", "e401k")], as.factor)

head(data)

lm = lm(pira ~ p401k + inc + incsq + age + agesq, data)
summary(lm)
```

First, we notice that the variable _pira_ cannot be encoded as a factor here, since the lm function works only on continuous dependent variables. We simply keep is as numerical. In the model output, we can see that the coefficient of _p401k_ is positive, but only marginally so. Still, the coefficient is statistically significantly different from 0 at the 1% significance level. This means that individuals enrolled in a 401k plan are 5.336% more likely (if the coefficients are interpreted as probabilities) to also have an IRA than individuals not enrolled in a 401k plan, on average, ceteris paribus.

## (b)
The linear probability model has the standard benefits of OLS and has better interpretability of the coefficients. According to OLS, however, the dependent variable is normally distributed (since the error terms are assumed to be). Obviously, a binary variable cannot be normally distributed. Consequently, the standard errors estimated by OLS cannot be right. Also, the variance of a binary variable depends on the values of the explanatory variables, so there is always heteroskedasticity. Here, we can correct the standard errors in the linear probability model using the robust option.\
When it comes to the interpretation as a probability, it also becomes clear that the fitted values can take on values below 0 and above 1. OLS does not place any restrictions here. Probabilities need to be constrained to be between 0 and 1. Moreover, according to the the OLS estimation, the partial effect of any explanatory variable is always constant. It may be the case, though, that changes in probability behave differently at different levels of the explanatory variable(s).

## (c) & (d)
```{r, message = FALSE}
summary(lm(e401k ~ p401k, dataOrig))
```

Variable z needs to satisfy two conditions to be considered an instrumental variable for x. __Instrumental exogeneity__ requires to covariance between instrument z and the error term u, which includes the unobservable potential confounders, to be 0. __Instrumental relevance__ is met when the covariance between instrument z and the endogenous explanatory variable x is not equal to 0. The second conditions is directly testable, while the first condition can only be supported by economic reasoning and perhaps by looking at the covariance between the instrument and other variables observed before the instrument.\  
In the case of the variable _e401K_, we can test the relevance condition quickly by running a regression of _e401k_ on _p401k_. Since the coefficient of p401k contains the variance between the two variables, we can see by the significance level whether the relevance condition is satisfied. We find that the coefficient 0.8399 is statistically significant at the 1% significance level and hence the relevance condition is met. For the exogeneity condition, we can only reason in terms of economics. To do this properly, though, we would need more information on what drives eligibility. Age, of course, might be one, but we are already controlling for that. So, at least to my mind, and without further information about the variables the exogeneity condition is satisfied.

## (e)
```{r, message = FALSE}
tslsm <- ivreg(pira ~ p401k + inc + incsq + age + agesq | 
  e401k + inc + incsq + age + agesq, data = data)
summary(tslsm, vcov = sandwich, df = Inf, diagnostics = TRUE)
msummary(list(OLS = lm, IV = tslsm))
modelplot(list(OLS = lm, IV = tslsm), coef_omit = "Intercept")
```

I decided to run the 2SLS estimation in one step here. That way, I can guarantee that the standard errors and t-statistics are correct.\
Unlike in the OLS regression, in this model (using robust standard errors), the coefficient of _p401k_ is not statistically significant at the 10% significance level. The coefficient magnitude decreased from 5.336% in the OLS regression to 2.070% in the 2SLS estimation. So, if the exogeneity condition were satisfied with certainty, the 2SLS estimation shows that there is no difference in IRA enrollment probability between individuals participating in a 401k plan or not, on average, ceteris paribus. The models can be compared in the table and plot above (the standard errors of the IV regression are not robust here).\
Looking at the diagnostic tests, we see that can reject the null hypothesis of a weak instrument in the weak instruments test. We also reject the null hypothesis in the Wu-Hausman test, indicating that OLS is not better here.


# 2.
```{r, message = FALSE}
library(margins)
```

## (a)
```{r, message = FALSE}
path = paste0(getwd(), "/loans.dta")
data = read_dta(path)
data[c("approve", "white", "male", "married", "dep", "sch", "cosign", "chist", "pubrec", 
  "mortlat1", "mortlat2", "vr")] = lapply(data[c("approve", "white", "male", "married", 
  "dep", "sch", "cosign", "chist", "pubrec", "mortlat1", "mortlat2", "vr")], as.factor)

head(data)

pm = glm(approve ~ white, data, family = binomial("probit"))
summary(pm)
predict(pm, newdata = data.frame("white" = as.factor(c(1, 0))), type = "response")
```

First, I encoded all focal variables, which I thought were categorical (I was not sure about _dep_), as factors. According to this crude estimated probit model of _approve_ on _white_, the probability of getting the loan approved increases when the subject is white compared to nonwhite, on average, since the coefficient is positive and statistically significant at the 1% significance level. The magnitude of this association cannot be estimated from this, though, since partial effects in probit models are non-constant. This model predicts a probability of loan approval of 90.8388% for whites and 70.7792% for nonwhites. This model is lacking some control variables, such as gender or age, though.

## (b)

```{r, message = FALSE}
dataLm = read_dta(path)
dataLm[c("white", "male", "married", "dep", "sch", "cosign", "chist", "pubrec", "mortlat1", 
  "mortlat2", "vr")] = lapply(dataLm[c("white", "male", "married", "dep", "sch", "cosign", 
  "chist", "pubrec", "mortlat1", "mortlat2", "vr")], as.factor)

lm = lm(approve ~ white, dataLm)
summary(lm)
predict(lm, newdata = data.frame("white" = as.factor(c(1, 0))))
```

For the linear probability model, we again need to make sure that the dependent variable in not encoded as a factor. This linear probability model predicts a 20.06% increase in probability of loan approval for whites (70.779% + 20.06% = 90.839%) compared to nonwhites (70.779%), on average. Both the intercept and the coefficient are statistically significant at the 1% significance level. The linear probability model makes the exact same predictions as the probit model. So in the case of this simple model, we prefer the linear probability model based on its better interpretability and OLS benefits. One can also notice a difference in standard errors between the two models.

## (c)
```{r, message = FALSE}
pm = glm(approve ~ white + hrat + obrat + loanprc + unem + male + married + dep + sch + 
  cosign + chist + pubrec + mortlat1 + mortlat2 + vr, data, family = binomial("probit"))
summary(pm)
```

The difference between whites and nonwhites persists in the extended model. The association between _white_ and _approve_ is still positive and statistically significant at the 1% significance level. This is evidence that there is still discrimination against nonwhites even after controlling for various variables.

(d)
```{r, message = FALSE}
logm = glm(approve ~ white + hrat + obrat + loanprc + unem + male + married + dep + sch + 
  cosign + chist + pubrec + mortlat1 + mortlat2 + vr, data, family = binomial("logit"))
summary(logm)

lm = lm(approve ~ white + hrat + obrat + loanprc + unem + male + married + dep + sch + 
  cosign + chist + pubrec + mortlat1 + mortlat2 + vr, dataLm)

# Probit marginal effects
margins(pm)
# Logit marginal effects
margins(logm)
# LPM marginal effects
margins(lm)
```

In the logit model, as well, the coefficient of _white_ is positive and statistically significant at the 1% significance level. The magnitude of the coefficient of _white_ is far smaller than in the probit model. This pattern can be observed for all other coefficient magnitudes and the standard errors, as well. The t-statistics are similar, though. Comparing the marginal effects (partial derivatives) of both models, we can see that they are quite similar. The marginal effects of the linear probability model diverge from the logit and probit estimates.


# 3.
```{r, message = FALSE}
library(readxl)
library(lubridate)
library(stats)
library(xts)
library(psych)
library(tseries)
library(forecast)
```

## (a)
```{r, message = FALSE}
path = paste0(getwd(), "/data_assignment2.xlsx")
data = lapply(excel_sheets(path), read_excel, path = path)

data = data[[1]]
data = data %>% mutate(DATE = ymd(DATE))

simpleToLog = function(returns) {return(log(returns + 1))}
for (i in (2:ncol(data))) {
  data[paste0("log",names(data)[i])] = simpleToLog(data[, i])
}

data = data.frame(data[,1], sapply(data[,(2:5)], unlist))
head(data)

acf(data$logEWRET, lag.max = 22)
pacf(data$logEWRET, lag.max = 22)
```

In the ACF plot, we can see that the functions decreases steeply from the first to the second lag and then gradually decreases towards statistical insignificance. Given that all of the first couple of autocorrelation coefficients are positive, this suggests that the AR coefficients should be positive, as well. The low persistence of the pattern suggests a lower number of AR coefficients, perhaps AR(1, 1). Still, all lags up until lag 10 lie outside the 95% confidence interval (and some additional ones thereafter).\
The PACF exhibits a more irregular pattern. The first lag is strongly different from 0 and the second one less so. The third and fourth one are strongly different from 0, again. Judging by the shear magnitude of the first lag, it could be an AR(1, 1) model, although MA terms could also play a role here. 
To better assess this, we should look at the information criteria. 

## (b)
```{r, message = FALSE}
row.names = c("AR(0)","AR(1)","AR(2)", "AR(3)", "AR(4)", "AR(5)")
column.names = c("MA(0)", "MA(1)", "MA(2)", "MA(3)", "MA(4)", "MA(5)")
matrix.names = c("AIC","SBIC")
table = array(NA, c(6, 6, 2), dimnames = list(row.names, column.names, matrix.names))

min_AIC = 10000
min_AIC_ARMA = c(0, 0)
min_SBIC = 10000
min_SBIC_ARMA = c(0, 0)
for (ar in 0:5) {
  for (ma in 0:5) {
    arma = arima(data$logEWRET, order = c(ar, 0, ma)) 
    table[ar + 1, ma + 1, 1] = AIC(arma)
    table[ar + 1, ma + 1, 2] = AIC(arma, k = log(nrow(data))) 
    if (AIC(arma) < min_AIC) {
      min_AIC = AIC(arma)
      min_AIC_ARMA = c(ar, ma)
    }
    if (AIC(arma, k = log(nrow(data))) < min_SBIC) {
      min_SBIC = AIC(arma, k = log(nrow(data)))
      min_SBIC_ARMA = c(ar, ma)
    }
  }
}

table
cat("The ARMA model with the lowest AIC value is ARMA(", toString(min_AIC_ARMA[1]), 
  ", ", toString(min_AIC_ARMA[2]), ")\n with an AIC value of ", toString(min_AIC), sep = "")
cat("The ARMA model with the lowest SBIC value is ARMA(", toString(min_SBIC_ARMA[1]), 
  ", ", toString(min_SBIC_ARMA[2]), ")\n with an SBIC value of ", toString(min_SBIC), sep = "")

AIC_model = arima(data$logEWRET, order = c(5, 0, 2))
AIC_model

SBIC_model = arima(data$logEWRET, order = c(1, 0, 2))
SBIC_model

# AIC Ljung-Box test
Box.test(AIC_model$residuals, lag = 10, type = "Ljung-Box")

# SBIC Ljung-Box test
Box.test(SBIC_model$residuals, lag = 10, type = "Ljung-Box")
```

The first table contains information about the AIC criterion, the second about the SBIC criterion. For each, we are looking for the ARMA model that minimizes the criterion. Accordingly, ARMA(5, 2) minimizes the AIC criterion with a value of -83476.6578. Perhaps, if we allowed more lags, the model would have been even more extensive. ARMA(1, 2) is the model that minimizes the SBIC criterion with a value of -83432.7178. A lower number of parameters allowed by the SBIC criterion is to be expected since the SBIC is the information criterion that imposes the strongest penalty (ln(T)) for each additional parameter that is included in the model.\
To check for possible model inadequacy we can perform the Box-Ljung test. I decided to run it with 10 lags, since the natural logarithm of the number of observations is about 9.4203. We fail to reject the null hypothesis of no residual serial correlation in the first 10 lags at the 5% significance level. For the SBIC model, we would reject the null hypothesis at the 10% significance level, though. This serves as strong evidence for model adequacy in the case of the AIC model and only mild evidence for model adequacy in the case of the SBIC model.

## (c)
```{r, message = FALSE}
# Alternatively, you could also use plot(forecast, include = 60, ...) or autoplot(forecast, include = 60, ...)
# But the date format in x axis is confusing and I did not find a good way to fix it.
# So I use a simple way to plot as following:

# Use a data.frame to also include Date as a variable

e1 = which(data$DATE == as.Date("1997-01-02"))
e2 = which(data$DATE == as.Date("2011-11-30"))

df_forecast = data.frame(Date = data$DATE, logEWRET = data$logEWRET)

# Restrict the sample to the required period
df_forecast = df_forecast[e1:(e2 + 21),] 

# Obtain the forecast results
f1 = forecast(arima(data$logEWRET[e1:e2], order = c(5,0,2)), 21)

# Get the predicted values, as well as the upper and lower limits of the 
# prediction interval
df_forecast$Predict = c(df_forecast$logEWRET[1:(e2 - e1 + 1)], f1$mean)
df_forecast$Upper   = c(rep(NA, e2-e1+1), f1$upper[,2])
df_forecast$Lower   = c(rep(NA, e2-e1+1), f1$lower[,2])

# Forecast estimates for the subsets
dff_subset          = df_forecast[seq(1,e2 - e1 + 1, by = 100),]
pred_subset         = df_forecast[(e2 - e1 + 2):nrow(df_forecast),]
ylimit              = range(pred_subset$Upper,pred_subset$Lower, finite = TRUE)
xlimit              = range(df_forecast$Date[3700: nrow(df_forecast)])

# Plot the log returns against the date
par(mfrow = c(2,1))
plot(logEWRET ~ Date, data = df_forecast[3700: nrow(df_forecast),], 
     type ="l", col="red", 
     xlim = xlimit, 
     ylim = ylimit, 
     xaxt = "n",
     ylab = "logEWRET",
     lwd  = 2)
# Shade the area between the upper and lower bound for the prediction period
polygon(c(pred_subset$Date,rev(pred_subset$Date)), 
        c(pred_subset$Upper, rev(pred_subset$Lower)),
        col=rgb(0,0,0.6,0.2), border=FALSE)
# Input the forecast estimates
par(new=TRUE)
plot(Predict ~ Date, data = df_forecast[(e2-e1+2): nrow(df_forecast),], type="l", 
     ylim = ylimit,  
     xlim = xlimit,
     xlab = "", ylab = "",
     col ="blue",
     xaxt = "n",
     yaxt = "n",
     lwd = 2)
axis.Date(side = 1, at = df_forecast$Date[3700: nrow(df_forecast)], 
          format = '%Y-%m-%d')

```

As we can see in the plot, both ARMA models quite quickly and in a similar fashion revert to their means, after about one day. The ARMA(5, 2) model selected by the AIC criterion shows perhaps a little more persistence. This pattern is to be expected, since forecasts become more inaccurate, the farther we predict into the future. Nevertheless, the actual observations rarely lie outside the confidence bands and, if so, barely. This returns some usefulness to the forecasts. The confidence bands of the 2 forecasts are almost identical.\
In terms of point forecast accuracy, the ARMA(1, 2) model does a slightly better job at forecasting given that both its accuracy measures are (just barely) lower. From this perspective and considering its parsimony, we would prefer this model over the ARMA(5, 2) model.

## (d)

### (i)
```{r, message = FALSE}
abs_residuals = abs(SBIC_model$residuals)
squared_residuals = SBIC_model$residuals^2

describe(data.frame(abs_residuals, squared_residuals))
```

### (ii)
```{r, message = FALSE}
acf(abs_residuals, lag.max = 22)
pacf(abs_residuals, lag.max = 22)

acf(squared_residuals, lag.max = 22)
pacf(squared_residuals, lag.max = 22)
```
If the model is adequate, then the residual series (not squared and not absolute) should behave like white noise. By performing the Ljung-Box test earlier we have already seen that this is likely the case. However, according to the plots here, there is still autocorrelation left in the second moment of the series. Both the squared and absolute residuals show persistent autocorrelation patterns. This information could be used in forecasting volatility using a GARCH model.

### (iii)
```{r, message = FALSE}
adf.test(abs_residuals)
adf.test(squared_residuals)
```

The null hypothesis of the Augmented Dickey-Fuller test is that the time series is non-stationary; that there is a unit root. For both the squared and absolute residuals we can reject this null hypothesis at the 1% significance level. We have evidence that both series are stationary.

### (iv)
```{r, message = FALSE}
row.names = c("AR(0)","AR(1)","AR(2)", "AR(3)", "AR(4)", "AR(5)", "AR(6)", "AR(7)", "AR(8)")
column.names = c("MA(0)", "MA(1)", "MA(2)", "MA(3)", "MA(4)", "MA(5)", "MA(6)", "MA(7)", "MA(8)")
table = array(NA, c(9, 9), dimnames = list(row.names, column.names))

min_SBIC = 10000
min_SBIC_ARMA = c(0, 0)
for (ar in 0:8) {
  for (ma in 0:8) {
    arma = arima(squared_residuals, order = c(ar, 0, ma)) 
    table[ar + 1, ma + 1] = AIC(arma, k = log(nrow(data))) 
    if (AIC(arma, k = log(nrow(data))) < min_SBIC) {
      min_SBIC = AIC(arma, k = log(nrow(data)))
      min_SBIC_ARMA = c(ar, ma)
    }
  }
}

table

cat("The ARMA model with the lowest SBIC value is ARMA(", toString(min_SBIC_ARMA[1]), ", ",
  toString(min_SBIC_ARMA[2]), ")\n with an SBIC value of ", toString(min_SBIC), sep = "")

residual_model = arima(squared_residuals, order = c(7, 0, 5))
residual_model

Box.test(residual_model$residuals, lag = 10, type = "Ljung-Box")
```

The SBIC criterion recommends an ARMA(7, 5) model for the squared residuals. I tested several ARMA specifications. At ARMA(8, 8) the AR components did not reach its capacity for the first time, and hence, I decided to stop there. Regarding the Box-Ljung test, we reject the null hypothesis of no residual serial correlation in the first 10 lags at the 1% significance level. This suggests that there could still be persistence in the squared residuals that is not captured by the current model. Perhaps an even higher order ARMA model could resolve this.


# 4.
```{r, message = FALSE}
library(vars)
```

## (a)
```{r, message = FALSE}
path = paste0(getwd(), "/data_assignment2.xlsx")
data = lapply(excel_sheets(path), read_excel, path = path)

data = data[[2]]

head(data)

adf.test(data$'1YRRATE')
adf.test(data$'5YRRATE')

toLog = function(vals) {return(c(NA, diff(log(vals))))}
for (i in (2:ncol(data))) {
  data[paste0("log",names(data)[i])] = toLog(data[[i]])
}

head(data)

adf.test(data$log1YRRATE[is.na(data$log1YRRATE) == FALSE])
adf.test(data$log5YRRATE[is.na(data$log1YRRATE) == FALSE])

data = data.frame(log1YRRATE = data$log1YRRATE[is.na(data$log1YRRATE) == FALSE], 
  log5YRRATE = data$log5YRRATE[is.na(data$log1YRRATE) == FALSE])

VARselect(data, lag.max = 10)

var = VAR(data, p = 3)
var
```

It is questionable whether the rates series are stationary. To check, we perform the Augmented Dickey-Fuller test for both. For both, we can reject the null hypothesis of non-stationarity at the 10% significance level, but we fail to do so for the 5-year rates at the 5% significance level. When we compute the log returns for both series and check again, we can reject the null hypothesis of non-stationarity in both cases at the 1% significance level. This step is similar to first-differencing. To find the appropriate number of lags for the VAR model, we apply the VARselect function, which returns the optimal number of lags according to 4 different criteria. 3 out of 4 indicate an optimal lag order of 3, so we go on to estimate the VAR(3) model. The model can be inspected in the output above.


## (b)
```{r, message = FALSE}
plot(irf(var, impulse = "log1YRRATE", response = c("log1YRRATE", "log5YRRATE"), 
  ortho = TRUE, n.ahead = 12, boot = TRUE, ci = 0.95, runs = 1000))
plot(irf(var, impulse = "log5YRRATE", response = c("log1YRRATE", "log5YRRATE"), 
  ortho = TRUE, n.ahead = 12, boot = TRUE, ci = 0.95, runs = 1000))

par(mar = c(0.5, 0.5, 0.5, 0.5))
plot(fevd(var, n.ahead = 12))

causality(var, cause = "log1YRRATE")$Granger
causality(var, cause = "log5YRRATE")$Granger
```
The response of the 5-year rate to a shock in the 1-year rate is statistically different from 0 at the zeroth lag, but becomes indistinguishable from 0 thereafter. Similarly, the response of the 1-year rate to a shock in itself is statistically different from 0 at the zeroth and perhaps slightly at the third lag.
The response of the 1-year rate to a shock in the 5-year rate is only visibly statistically different from 0 at the first lag, while the 5-year rate response to a shock in itself is statistically different from 0 at the zeroth and perhaps at the first lag.\
So interestingly, the 5-year rate shows some sort of response to a shock in the 1-year rate but not the other way around. A similar picture emerges when we plot the variance decomposition. The variance of the 1-year rates is almost exclusively explained by itself, but for the 5-year rate only half of the variance is explained by itself. We should remember that the ordering of the variables has an effect on the impulse responses. In fact, after swapping the variables in the data frame above, we observe the same pattern, just the other way around.\
Looking at Granger causality, we can reject both the hypothesis that _log1YRRATE_ does not Granger-cause _log5YRRATE_ and that _log5YRRATE_ does not Granger-cause _log1YRRATE_ at the 1% significance level.
So, overall, the variables seem to be intertwined with no striking leaning towards one variable or the other.
